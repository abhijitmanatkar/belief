{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e2b431-6686-4532-9461-b5a53ab1adcd",
   "metadata": {},
   "source": [
    "## Experiment 3: Using WDIK as feedback and constraint solving within the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a60b23bc-d254-4b2b-a7b9-eee48b2f72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707f4b59-9983-4bb9-bab9-10fb8409bd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import experiment_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3417b2b3-b3e3-438c-a978-84bdb79fec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from belief.utils import load_macaw, load_tokenizer\n",
    "from belief.utils import macaw_input, run_macaw, get_macaw_scores, get_macaw_outs\n",
    "from belief.evaluation import load_facts\n",
    "from belief.nli import load_nli_model, load_nli_tokenizer, run_nli\n",
    "from belief.lmbb import Proposition, LMBB\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "from z3 import *\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f7060c-b603-44c3-9d85-0777c4c8274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = load_facts('data/calibration_facts.json', num_batches=1)[0]\n",
    "\n",
    "with open('cache/wdik.json', 'r') as f:\n",
    "    wdik = json.load(f)\n",
    "\n",
    "with open('data/constraints_v2.json', 'r') as f:\n",
    "    constraint_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "811451f5-9fcf-4a81-b5eb-a6e15416d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_by_entity = defaultdict(list)\n",
    "for fact in facts:\n",
    "    facts_by_entity[fact.subject].append(fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14c9daec-265e-43e0-b9ee-737ed5e0526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = LMBB(\n",
    "    model=None, \n",
    "    tokenizer=None, \n",
    "    raw_constraints=constraint_data['links'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1569e934-e6b9-4daa-a49c-f4dc579df1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home2/abhijit.manatkar/miniconda3/envs/advnlp did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home2/abhijit.manatkar/.nvm')}\n",
      "  warn(msg)\n",
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/abhijit/.deno')}\n",
      "  warn(msg)\n",
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home2/abhijit.manatkar/gems')}\n",
      "  warn(msg)\n",
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/Modules/modulefiles'), PosixPath('/opt/Modules/$MODULE_VERSION/modulefiles')}\n",
      "  warn(msg)\n",
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  eval `/opt/Modules/$MODULE_VERSION/bin/modulecmd bash $*`\\n}')}\n",
      "  warn(msg)\n",
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('experiments/108d2f0b-9f8a-4df1-8edd-0af4fe5a567b')}\n",
      "  warn(msg)\n",
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = load_macaw()\n",
    "tokenizer = load_tokenizer()\n",
    "\n",
    "nli_model = load_nli_model()\n",
    "nli_tokenizer = load_nli_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee75dd3a-5bc6-4deb-a787-eb78a6103bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z3mult = 1000\n",
    "wdik_weight = 1.5\n",
    "NUM_FACTS = 3\n",
    "NUM_CONSTRAINTS = 30\n",
    "constraint_mult = 1\n",
    "nn = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9633b5-479d-4ce3-bc93-5bc976a65404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Entity = computer ###\n",
      "Getting initial outs from QA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:14<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MaxSAT problem...\n",
      "Solving MaxSAT problem...\n",
      "F1 = 0.8888888832921811, Consistency = 0.8461538461538461\n",
      "\n",
      "### Entity = ape ###\n",
      "Getting initial outs from QA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:14<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MaxSAT problem...\n",
      "Solving MaxSAT problem...\n"
     ]
    }
   ],
   "source": [
    "yes_no = ['yes', 'no']\n",
    "\n",
    "cumulative_ground_truth = []\n",
    "\n",
    "for i, entity in enumerate(list(facts_by_entity.keys())):\n",
    "    \n",
    "    print(f\"### Entity = {entity} ###\")\n",
    "    \n",
    "    initial_outs = {}\n",
    "    \n",
    "    print(\"Getting initial outs from QA model...\")\n",
    "    for fact in tqdm(facts_by_entity[entity][:nn]):\n",
    "        question = fact.get_question()\n",
    "        context = ' '.join(random.sample(wdik[entity], NUM_FACTS))\n",
    "        inp_str = macaw_input(question=question, options=yes_no, context=context, targets='A')\n",
    "        initial_outs[fact.sentence] = get_macaw_scores(inp_str, yes_no, model, tokenizer)\n",
    "    \n",
    "    # print(\"Creating MaxSAT problem...\")\n",
    "    optim = Optimize()\n",
    "    bools = {}\n",
    "    for sent in initial_outs:\n",
    "        bools[sent] = Bool(sent)\n",
    "        optim.add_soft(bools[sent], int(initial_outs[sent]['yes'] * z3mult))\n",
    "        optim.add_soft(Not(bools[sent]), int(initial_outs[sent]['no'] * z3mult))\n",
    "    \n",
    "    constraints = []\n",
    "    \n",
    "    for fact1 in facts_by_entity[entity][:nn]:\n",
    "        for fact2 in facts_by_entity[entity][:nn]:\n",
    "            if fact1.sentence == fact2.sentence:\n",
    "                continue\n",
    "            \n",
    "            assertion1 = fact1.get_assertion()\n",
    "            assertion2 = fact2.get_assertion()\n",
    "            nli_outs = run_nli(\n",
    "                premise=assertion1, \n",
    "                hypothesis=assertion2, \n",
    "                model=nli_model, \n",
    "                tokenizer=nli_tokenizer\n",
    "            )\n",
    "            \n",
    "            constraints.append({\n",
    "                \"type\": \"entailment\",\n",
    "                \"src\": fact1.sentence,\n",
    "                \"dest\": fact2.sentence,\n",
    "                \"weight\": nli_outs['entailment']\n",
    "            })\n",
    "            \n",
    "            constraints.append({\n",
    "                \"type\": \"contradiction\",\n",
    "                \"src\": fact1.sentence,\n",
    "                \"dest\": fact2.sentence,\n",
    "                \"weight\": nli_outs['contradiction']\n",
    "            })\n",
    "    \n",
    "    constraints.sort(key=lambda c : c['weight'])\n",
    "    for constraint in constraints[:NUM_CONSTRAINTS]:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    updated_beliefs = {}     \n",
    "    \n",
    "    print(\"Solving MaxSAT problem...\")\n",
    "    optim.check()\n",
    "    mod = optim.model()\n",
    "    \n",
    "    for fact in facts_by_entity[entity][:nn]:\n",
    "        new_fact = copy.deepcopy(fact)\n",
    "        new_fact.boolean = bool(mod.evaluate(bools[fact.sentence]))\n",
    "        updated_beliefs[new_fact.sentence] = new_fact\n",
    "        \n",
    "    evaluator.set_beliefs(updated_beliefs)\n",
    "    cumulative_ground_truth += facts_by_entity[entity][:nn]\n",
    "    f1 = evaluator.calculate_f1(cumulative_ground_truth)\n",
    "    consistency = evaluator.calculate_consistency()\n",
    "    print(f\"F1 = {f1}, Consistency = {consistency}\")\n",
    "    print()\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advnlpkernel",
   "language": "python",
   "name": "advnlpkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
