{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f16a90ea-fd73-4dd7-98b6-ca7457564749",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7a982bf-1452-4ee2-b270-fa78dc6d90da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration, T5Config, AutoModelForSequenceClassification\n",
    "from belief.evaluation import load_facts\n",
    "import random\n",
    "import string\n",
    "from belief.macaw_utils import decompose_slots, compute_answer, run_model_with_outputs\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "711887c2-310c-4ad7-b5ac-73ce831deaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "257aba14-3c9b-448d-bca3-046e05ac5b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda116.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda116.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home2/abhijit.manatkar/miniconda3/envs/advnlp did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home2/abhijit.manatkar/.nvm')}\n",
      "  warn(msg)\n",
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/abhijit/.deno')}\n",
      "  warn(msg)\n",
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home2/abhijit.manatkar/gems')}\n",
      "  warn(msg)\n",
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/Modules/$MODULE_VERSION/modulefiles'), PosixPath('/opt/Modules/modulefiles')}\n",
      "  warn(msg)\n",
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  eval `/opt/Modules/$MODULE_VERSION/bin/modulecmd bash $*`\\n}')}\n",
      "  warn(msg)\n",
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/macaw-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/macaw-large\", load_in_8bit=True, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fe7b816-c6bf-46d8-89ed-499b8bb61610",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a8f1ad7-1c15-4387-8291-2bfae37f1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = load_facts('./data/calibration_facts.json', num_batches=1)[0]\n",
    "silver_facts = load_facts('./data/silver_facts.json', num_batches=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f76a1f50-cf00-4128-b3de-2a4fac4162ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macaw_input(question=\"\", answer=\"\", options=(), explanation=\"\", context=\"\", targets='AE'):\n",
    "    \n",
    "    if len(question) > 0:\n",
    "        question_str = '$question$ = ' + question  + \" ; \"\n",
    "    elif 'Q' in targets:\n",
    "        question_str = \"$question$ ; \"\n",
    "    \n",
    "    if len(explanation) > 0:\n",
    "        explanation_str = \"$explanation$ = \" + explanation + \" ; \"\n",
    "    elif 'E' in targets:\n",
    "        explanation_str = \"$explanation$ ; \"\n",
    "    else:\n",
    "        explanation_str = \"\"\n",
    "    \n",
    "    if len(answer) > 0:\n",
    "        answer_str = \"$answer$ = \" + answer \n",
    "        if len(context) > 0:\n",
    "            answer_str += \" ; \"\n",
    "    elif 'A' in targets:\n",
    "        answer_str = \"$answer$\"\n",
    "        if len(context) > 0:\n",
    "            answer_str += \" ; \"\n",
    "    else:\n",
    "        answer_str = \"\"\n",
    "    \n",
    "    if len(context) > 0:\n",
    "        context_str = \"$context$ = \" + context\n",
    "    else:\n",
    "        context_str = \"\"\n",
    "    \n",
    "    letters = list(string.ascii_uppercase)\n",
    "    if len(options) > 0:\n",
    "        option_str = \"$mcoptions$ = \"\n",
    "        for letter, option in zip(letters, options):\n",
    "            option_str += f\"({letter}) {option} \"\n",
    "        option_str += \"; \"\n",
    "    elif 'M' in targets:\n",
    "        option_str = \"$mcoptions$ ; \"\n",
    "    else:\n",
    "        option_str = \"\"\n",
    "    \n",
    "    return f\"{question_str}{explanation_str}{option_str}{answer_str}{context_str}\"\n",
    "\n",
    "def run_macaw(input_str, model, tokenizer):\n",
    "    input_ids = tokenizer.encode(input_str, return_tensors=\"pt\").to(device)\n",
    "    outs = model.generate(input_ids, max_length=500, early_stopping=True)\n",
    "    return tokenizer.batch_decode(outs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0caa2af2-5466-466f-a33c-3244f7fe3f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(albatross,IsA,expert, False, -99999.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact = random.choice(facts)\n",
    "fact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951620f9-3870-4930-8481-e0cf90edbad5",
   "metadata": {},
   "source": [
    "### Facts generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cba2fd7-a300-4dbb-9e73-dc3e53970d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(entity):\n",
    "\n",
    "    a_an = \"an\" if entity[0] in ['a', 'e', 'i', 'o', 'u'] else \"a\"\n",
    "    \n",
    "    what_is = f\"What is {a_an} {entity}?\"\n",
    "    made_of = f\"What is {a_an} {entity} made of?\"\n",
    "    capable_of = f\"What is {a_an} {entity} capable of?\"\n",
    "    has_what_part = f\"What parts does {a_an} {entity} have?\"\n",
    "    has_what_property = f\"What properties does {a_an} {entity} have?\"\n",
    "    which_category = f\"Which category does {a_an} {entity} belong to?\"\n",
    "\n",
    "    return [what_is, made_of, capable_of, has_what_part, has_what_property, which_category]\n",
    "\n",
    "def get_qa_pairs(entity):\n",
    "    \n",
    "    questions = get_questions(entity)\n",
    "\n",
    "    inpstrs = [macaw_input(targets='AE', question=question) for question in questions]\n",
    "    inpids = tokenizer(inpstrs, truncation=True, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    num_beams = 3\n",
    "    num_return_sequences = 3\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=inpids, \n",
    "        max_length=500,\n",
    "        num_beams=num_beams,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    out_text = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "    \n",
    "    qas = set()\n",
    "    for i,q in enumerate(questions):\n",
    "        for j in range(3*i, 3*(i+1)):\n",
    "            out_str = out_text[j]\n",
    "            slots = decompose_slots(out_str)\n",
    "            ans = slots['answer'] if 'answer' in slots else ''\n",
    "            qas.add((q,ans))\n",
    "    \n",
    "    return list(qas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a572aadf-66d8-4bad-819a-11d1bb638dbc",
   "metadata": {},
   "source": [
    "### QA to Declarative Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eba9b22f-a994-4bcf-8bc0-2cb52271b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load('./t5-statement-conversion-finetune.pt')['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9c6f4c5-d342-405a-89a1-6f48be00b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = T5Config.from_pretrained('t5-base')\n",
    "qads = T5ForConditionalGeneration(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "628a56fc-75d6-40cd-9ad7-4c1c5c0bd099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qads.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe1f9fa2-3173-48fd-adf9-f00bda58caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qads(qas):\n",
    "    inpstrs = [q + \" \" + a for (q,a) in qas]\n",
    "    inpids = tokenizer(inpstrs, truncation=True, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    out = qads.generate(inpids, max_length=500)\n",
    "    out_text = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5e911b1-0a9e-45bc-bf8f-aeb045d5a1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An ant has specialized cells for feeding on insects.',\n",
       " 'An ant has properties of cells.',\n",
       " 'An ant belongs to the predator category.',\n",
       " 'An ant has eyes, nymphs, and body.',\n",
       " 'An ant is capable of reproduction.',\n",
       " 'An ant is an animal.',\n",
       " 'An ant is made of cellulose.',\n",
       " 'An ant has eyes, nymphs, and larvae parts.',\n",
       " 'An ant belongs to the category predators.',\n",
       " 'An ant is capable of stinging.',\n",
       " 'An ant has specialized cells for protection.',\n",
       " 'An ant is capable of sex reproduction.',\n",
       " 'An ant is a kind of worm.',\n",
       " 'An ant belongs to the category of insects.',\n",
       " 'An ant is made of venom.',\n",
       " 'An ant is a kind of lizard.',\n",
       " 'An ant has specialized specialized cells for healing.',\n",
       " 'An ant is made of specialized cells']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qas = get_qa_pairs('ant')\n",
    "run_qads(qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f0c7635-a420-4d50-a40e-ab004a12a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = list(set([fact.subject for fact in facts]) | set([fact.subject for fact in silver_facts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c1c4c2d-c343-465b-88b0-20180e86e11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating WDIK statements in 1019.94 seconds\n"
     ]
    }
   ],
   "source": [
    "ent_facts = {}\n",
    "\n",
    "start_t = time.time()\n",
    "for ent in ents:\n",
    "    qas = get_qa_pairs(ent)\n",
    "    decs = run_qads(qas)\n",
    "    ent_facts[ent] = decs\n",
    "end_t = time.time()\n",
    "\n",
    "print(f'Finished generating WDIK statements in {round(end_t - start_t, 2)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "87650e56-1606-48b9-aeac-1bc242f31487",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./wdik.json', 'w') as f:\n",
    "    json.dump(ent_facts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28143123-3994-4154-bf44-f8f78081a01d",
   "metadata": {},
   "source": [
    "### NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "de74f810-f783-4abd-9427-72a68b9f284d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/abhijit.manatkar/miniconda3/envs/advnlp/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nli_tokenizer = AutoTokenizer.from_pretrained(\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\")\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "898d9f24-73af-4253-bd5b-b8ff838cf9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nli(premise, hypothesis):\n",
    "    input_ids = nli_tokenizer.encode(premise, hypothesis, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    output = nli_model(input_ids)\n",
    "\n",
    "    prediction = torch.softmax(output.logits[0], -1).tolist()\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    prediction = {name: round(float(pred) , 3) for pred, name in zip(prediction, label_names)}\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "73f5b0c3-3477-4c0a-9145-af2f672c7ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ant is not a plastic.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact.get_nl_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ac357ba2-aac9-45aa-806a-5c9fec9cc312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ant,IsA,road, False, -99999.0)\n",
      "\n",
      "An ant is a kind of worm. -> ant is not a road.\n",
      "{'entailment': 0.689, 'neutral': 0.141, 'contradiction': 0.17}\n",
      "\n",
      "ant is not a road. -> An ant is a kind of worm.\n",
      "{'entailment': 0.004, 'neutral': 0.989, 'contradiction': 0.007}\n",
      "\n",
      "An ant is made of venom. -> ant is not a road.\n",
      "{'entailment': 0.177, 'neutral': 0.275, 'contradiction': 0.548}\n",
      "\n",
      "ant is not a road. -> An ant is made of venom.\n",
      "{'entailment': 0.001, 'neutral': 0.988, 'contradiction': 0.011}\n",
      "\n",
      "An ant is capable of sex reproduction -> ant is not a road.\n",
      "{'entailment': 0.221, 'neutral': 0.449, 'contradiction': 0.33}\n",
      "\n",
      "ant is not a road. -> An ant is capable of sex reproduction\n",
      "{'entailment': 0.003, 'neutral': 0.989, 'contradiction': 0.008}\n",
      "\n",
      "An ant has specialized cells for feeding and protection. -> ant is not a road.\n",
      "{'entailment': 0.563, 'neutral': 0.159, 'contradiction': 0.279}\n",
      "\n",
      "ant is not a road. -> An ant has specialized cells for feeding and protection.\n",
      "{'entailment': 0.001, 'neutral': 0.998, 'contradiction': 0.001}\n",
      "\n",
      "An ant has specialized cells for protection. -> ant is not a road.\n",
      "{'entailment': 0.423, 'neutral': 0.235, 'contradiction': 0.342}\n",
      "\n",
      "ant is not a road. -> An ant has specialized cells for protection.\n",
      "{'entailment': 0.001, 'neutral': 0.998, 'contradiction': 0.001}\n",
      "\n",
      "An ant belongs to the category of insects. -> ant is not a road.\n",
      "{'entailment': 0.901, 'neutral': 0.084, 'contradiction': 0.015}\n",
      "\n",
      "ant is not a road. -> An ant belongs to the category of insects.\n",
      "{'entailment': 0.782, 'neutral': 0.215, 'contradiction': 0.003}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact = random.choice(facts)\n",
    "print(fact)\n",
    "print()\n",
    "qas = get_qa_pairs(fact.subject)\n",
    "wdik = run_qads(qas)\n",
    "\n",
    "for wik in wdik:\n",
    "    prop = fact.get_nl_sentence()\n",
    "    print(wik + \" -> \" + prop)\n",
    "    print(run_nli(premise=wik, hypothesis=prop))\n",
    "    print()\n",
    "    print(prop + \" -> \" + wik)\n",
    "    print(run_nli(premise=prop, hypothesis=wik))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ad77c4a8-284e-4362-9b3a-267d5490acda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premises: ['An albatross is a sea animal.', 'An albatross is made of feathers.', 'An albatross is capable of laying eggs.', 'An albatross has gills, talons, and feet.', 'An albatross has blubber and fur.', 'An albatross belongs to the mammals category.']\n",
      "Hypothesis: albatross is capable of fly.\n"
     ]
    }
   ],
   "source": [
    "print(\"Premises:\", wdik)\n",
    "print(\"Hypothesis:\", fact.get_nl_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b5efaebc-5b8d-4741-8fdb-423afb7bca34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': 0.004, 'neutral': 0.994, 'contradiction': 0.002}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_nli(wdik[1], fact.get_nl_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4872883f-23c8-41f1-b161-55e3b74a9fe0",
   "metadata": {},
   "source": [
    "### Macaw scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0c933d2-f741-4214-b87c-8b399f8edf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_strings_from_options(options):\n",
    "    l = []\n",
    "    for option in options:\n",
    "        l.append((f\"$answer$ = {option}\", option))\n",
    "    return l\n",
    "\n",
    "def get_macaw_scores(input_string, options):\n",
    "    out_str = get_output_strings_from_options(options)\n",
    "    res = run_model_with_outputs(model, tokenizer, device, inp_str, out_str)\n",
    "    scores = {}\n",
    "    for r in res:\n",
    "        scores[r['output_text']] = r['score']\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f21cb833-96e1-410b-96f8-07f529d9403f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ant,IsA,candy, False, -99999.0)\n",
      "Is a ant a candy?\n",
      "$question$ = Is a ant a candy? ; $mcoptions$ = (A) yes (B) no ; $answer$\n",
      "{'yes': 0.47121472070395004, 'no': 0.997046073721489}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['$answer$ = no']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop = random.choice(facts)\n",
    "print(prop)\n",
    "\n",
    "question = prop.get_nl_question()\n",
    "print(question)\n",
    "\n",
    "yesno = (\"yes\", \"no\")\n",
    "yesno2 = [(\"$answer$ = yes\", \"yes\"), (\"$answer$ = no\", \"no\")]\n",
    "\n",
    "yesnomaybe = (\"yes\", \"no\", \"maybe\")\n",
    "\n",
    "options = (\"yes\", \"no\")\n",
    "\n",
    "inp_str = macaw_input(question=question, options=options, targets='A')\n",
    "print(inp_str)\n",
    "\n",
    "# compute_answer(model, tokenizer, device, inp_str, generator_options={})\n",
    "\n",
    "# res = run_model_with_outputs(model, tokenizer, device, inp_str, yesno2)\n",
    "# scores = {}\n",
    "# for r in res:\n",
    "#     scores[r['output_text']] = r['score']\n",
    "# print(scores)\n",
    "# print()\n",
    "\n",
    "print(get_macaw_scores(inp_str, options))\n",
    "\n",
    "run_macaw(inp_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a387427f-3082-4723-bb71-95b436943c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from belief.utils import noun_fluenterer\n",
    "with open('belief/templates.json', 'r') as f:\n",
    "    templates = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "29e2fd70-20a4-4bcd-860c-74c2898087a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(daffodil,HasPart,floral leaf, True, -99999.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'daffodil has floral leaf.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact = random.choice(facts)\n",
    "print(fact)\n",
    "fact.get_positive_assertion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "45bc51dc-c96e-4e54-a0fa-b333911fd0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a daffodil has a floral leaf.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advnlpkernel",
   "language": "python",
   "name": "advnlpkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
